{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEMA Lifeline\n",
    "\n",
    "### Google places nearby EDA\n",
    "\n",
    "Gathering google data via Google Places_Nearby API and Google Elevation API\n",
    "\n",
    "By: John Wertz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import googlemaps\n",
    "import requests\n",
    "import time\n",
    "import ast\n",
    "import math\n",
    "\n",
    "apikey = 'ZIzaSyBZGKnHrgP7yNXmzxy8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate google maps \n",
    "#gmaps = googlemaps.Client(key=apikey)\n",
    "\n",
    "#gmaps.places_nearby(location = loc,radius=rad, type=place_types,page_token=None) # Template\n",
    "#This idea came from (https://github.com/meldev00/FEMA_disaster_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Definition Sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place Type that are returned in places_nearby API. Complete list\n",
    "\n",
    "google_place_type_list = ['accounting', 'airport', 'amusement_park', 'aquarium', 'art_gallery', 'atm', 'bakery',\n",
    "                          'bank', 'bar', 'beauty_salon', 'bicycle_store', 'book_store', 'bowling_alley',\n",
    "                          'bus_station', 'cafe', 'campground', 'car_dealer', 'car_rental', 'car_repair', \n",
    "                          'car_wash', 'casino', 'cemetery', 'church', 'city_hall', 'clothing_store', \n",
    "                          'convenience_store', 'courthouse', 'dentist', 'department_store', 'doctor', 'drugstore', \n",
    "                          'electrician', 'electronics_store', 'embassy', 'fire_station', 'florist', 'funeral_home',\n",
    "                          'furniture_store', 'gas_station', 'grocery_or_supermarket', 'gym', 'hair_care', \n",
    "                          'hardware_store', 'hindu_temple', 'home_goods_store', 'hospital', 'insurance_agency', \n",
    "                          'jewelry_store', 'laundry', 'lawyer', 'library', 'light_rail_station', 'liquor_store', \n",
    "                          'local_government_office', 'locksmith', 'lodging', 'meal_delivery', 'meal_takeaway', \n",
    "                          'mosque', 'movie_rental', 'movie_theater', 'moving_company', 'museum', 'night_club', \n",
    "                          'painter', 'park', 'parking', 'pet_store', 'pharmacy', 'physiotherapist', 'plumber', \n",
    "                          'police', 'post_office', 'primary_school', 'real_estate_agency', 'restaurant', \n",
    "                          'roofing_contractor', 'rv_park', 'school', 'secondary_school', 'shoe_store', \n",
    "                          'shopping_mall', 'spa', 'stadium', 'storage', 'store', 'subway_station', 'supermarket', \n",
    "                          'synagogue', 'taxi_stand', 'tourist_attraction', 'train_station', 'transit_station', \n",
    "                          'travel_agency', 'university', 'veterinary_care', 'zoo'\n",
    "                          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of type to remove from the types data\n",
    "#    note: this data is not found in the google master list\n",
    "exclude_list = ['point_of_interest',\n",
    "                'establishment'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dictionary: fema_lifeline_dictionary from the data file\n",
    "\n",
    "with open(\"../data/fema_lifeline_dictionary.txt\",\"r\") as data:\n",
    "    fema_lifeline_dictionary = ast.literal_eval(data.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list: place_type_list\n",
    "\n",
    "with open(\"../data/place_type_list.txt\",\"r\") as data:\n",
    "    place_type_list = ast.literal_eval(data.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: get_place_nearby\n",
    "\n",
    "# Paramters to pass:\n",
    "#      user_location ( a string that contains lat, long geo location as '39.7392, -104.9903'\n",
    "#      user_radius  ( in meters: 5 miles = 8,046.72)\n",
    "#      place_type_list ( a list of google defined place type to retrieve on)\n",
    "#      user_apikey. ( the all important api key info *top secret*)\n",
    "\n",
    "def get_place_nearby (user_location, user_radius, place_type_list, user_apikey):\n",
    "    \n",
    "    # Instantiate google maps \n",
    "    gmaps = googlemaps.Client(key=user_apikey)\n",
    "    \n",
    "    # Initialize \n",
    "    saved_results = pd.DataFrame()\n",
    "    ndx_count = 0\n",
    "    ndx_call = 0\n",
    "    ndx_scale = 1\n",
    "    columns_to_drop_list = ['icon', 'id',  'plus_code', 'opening_hours', 'photos', 'place_id', 'rating', 'reference', \n",
    "                            'scope', 'user_ratings_total', 'price_level']\n",
    "    \n",
    "    # loop through place_type_list and perform place_nearby call with the type desscrition in params\n",
    "    for type_description in place_type_list:\n",
    "\n",
    "        places_nearby_info = gmaps.places_nearby(location = user_location, \n",
    "                                                 radius = user_radius, \n",
    "                                                 type = type_description)\n",
    "        \n",
    "        # get count of results info (rows) and increment count of call\n",
    "        ndx_call += 1\n",
    "        ndx_count = ndx_count + len(places_nearby_info['results'])\n",
    "        \n",
    "        # create data frame to store info\n",
    "        results = pd.DataFrame(places_nearby_info['results'])\n",
    "        \n",
    "        # remove info that we will not need from call results\n",
    "        # first need to find if column exist, before dropping. Prevents errors.\n",
    "        columns_to_drop = []\n",
    "        for x in columns_to_drop_list:\n",
    "            if x in results.columns:\n",
    "                # add to drop list\n",
    "                columns_to_drop.append(x)\n",
    "\n",
    "        results.drop(columns=columns_to_drop, inplace = True)\n",
    "\n",
    "        # add results to saved_results and loop back to get next data \n",
    "        saved_results = saved_results.append(results, ignore_index = True, sort=False)\n",
    "        \n",
    "        # print out message for results count\n",
    "        if ndx_count > (100 * ndx_scale):\n",
    "            print(f'{ndx_call} calls and {ndx_count} rows of data collected')\n",
    "            ndx_scale = int(round(ndx_count/100, 0)) + 1\n",
    "            \n",
    "        # to prevent overloading the call to google places_nearby\n",
    "        time.sleep(3)\n",
    "        \n",
    "        \n",
    "    # Save info in a data file\n",
    "    print(f'{ndx_count} rows of data collected')\n",
    "    \n",
    "    return saved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function get_distance\n",
    "\n",
    "# Haversine formula for finding the distance between two lat, long. This code follows the example from Wayne Dyck.\n",
    "#   Link: https://gist.github.com/rochacbruno/2883505 \n",
    "\n",
    "def get_distance(lat_orgin, long_orgin, lat_destination, long_destination):\n",
    "\n",
    "    radius = 6378  #The radius of Earth in Kilometers is 6378\n",
    "    \n",
    "    length_lat = math.radians(lat_destination - lat_orgin)\n",
    "    length_long = math.radians(long_destination - long_orgin)\n",
    "    \n",
    "    a = (math.sin(length_lat/2))**2 + math.cos(math.radians(lat_orgin)) * \\\n",
    "            math.cos(math.radians(lat_destination)) * math.sin(length_long/2)**2\n",
    "    \n",
    "    distance = radius * (2 * math.atan2(math.sqrt(a), math.sqrt(1-a)))\n",
    "    \n",
    "    return round(distance, 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: assign_lifeline\n",
    "\n",
    "def assign_lifeline (types, list_data, number):\n",
    "#     # convert types data from string into a list\n",
    "#     types = types[1:len(types) - 1].replace(\"'\", \"\").split(\", \")\n",
    "    \n",
    "    for x in types:\n",
    "        if x in list_data:\n",
    "            return number\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: cleanup_types\n",
    "\n",
    "def cleanup_types (types, exclude_list):\n",
    "    # convert types data from string into a list\n",
    "    types = types[1:len(types) - 1].replace(\"'\", \"\").split(\", \")\n",
    "    \n",
    "    # remove unnecessary type data\n",
    "    for x in exclude_list:\n",
    "        if x in types:\n",
    "            types.remove(x)\n",
    "    \n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: set_duplicate_flag\n",
    "\n",
    "# check the duplicate list with ndx. if found return 'D' flag for duplicate found, 'S' for single.\n",
    "# geometry data ==> ndx\n",
    "# df_dup ===> duplicate_list    a subset of data that contain duplicate rows based on geo info   \n",
    "\n",
    "def set_duplicate_flag(ndx, duplicate_list):\n",
    "\n",
    "    if ndx in duplicate_list:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'S'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: analyze_duplicate\n",
    "\n",
    "# This function will evaluate the duplicate rows and choose which one to keep by the hierachy of the\n",
    "#     type information and lifeline number: \n",
    "# Will return the duplicate_flag key as follows:\n",
    "#    's' : single\n",
    "#    'r' : remove\n",
    "#\n",
    "# Hierachy:\n",
    "#    1) 'gas_station' over all  ==> 4\n",
    "#    2) 'drugstore' over 'food' ==> 3\n",
    "#    3) 'hospital' over 'airport' ==> 3    \n",
    "#    4) 'hospital' over 'local_government_office' ==> 3  \n",
    "#    5) 'hospital' over 'university' ==> 3  \n",
    "#    6) 'hardware_store' over 'car_dealer' ==> 2  \n",
    "#    7) 'restaurant' over 'car_repair' ==> 2  \n",
    "#\n",
    "# pass in row_types         ==> types info the df_data\n",
    "# pass in row_lifeline_ndx  ==> lifeline_number the df_data\n",
    "# pass in df                ==> df_data\n",
    "\n",
    "def analyze_duplicate(row):\n",
    "\n",
    "    if row['duplicate_flag'] =='D':\n",
    "        \n",
    "        types = row['types']\n",
    "#        print(type(types))\n",
    "#        print(types)\n",
    "#        types = types[1:len(types) - 1].replace(\"'\", \"\").split(\", \")        \n",
    "        \n",
    "        # Hierachy 1\n",
    "        if 'gas_station' in types and row['lifeline_number'] == 4:      \n",
    "            return 'S'\n",
    "        # Hierachy 2\n",
    "        elif 'drugstore' in types and 'food' in types and row['lifeline_number'] == 3:       \n",
    "            return 'S'\n",
    "        # Hierachy 3\n",
    "        elif 'hospital' in types and 'airport' in types and row['lifeline_number'] == 3:                  \n",
    "            return 'S'\n",
    "        # Hierachy 4\n",
    "        elif 'hospital' in types and 'local_government_office' in types and row['lifeline_number'] == 3:\n",
    "            return 'S'\n",
    "        # Hierachy 5\n",
    "        elif 'hospital' in types and 'university' in types and row['lifeline_number'] == 3:     \n",
    "            return 'S'\n",
    "        # Hierachy 6\n",
    "        elif 'hardware_store' in types and 'car_dealer' in types and row['lifeline_number'] == 2:     \n",
    "            return 'S'\n",
    "        # Hierachy 7\n",
    "        elif 'restaurant' in types and 'car_repair' in types and row['lifeline_number'] == 2:     \n",
    "            return 'S'\n",
    "        else:\n",
    "            return 'R'\n",
    "\n",
    "    else:\n",
    "        return 'S'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: double_check_duplicate_keys\n",
    "\n",
    "# function to double check to make sure all duplicates have been addressed\n",
    "#     funtionality: \n",
    "# df_check ==> df    contains subset of data where duplicate flag set and stay flag set \n",
    "# df_data  ==> row   row from data for evaluation\n",
    "#\n",
    "# Get the row's geo info when the row flag has duplicate flag set and removal flag set.\n",
    "# Find matching geo info in df. \n",
    "#    If found, set checked flag to stay. \n",
    "#    If not found, set checked flag to missed.\n",
    "# All else set checked flag to stay.\n",
    "\n",
    "def double_check_duplicate_keys(row, df):\n",
    "\n",
    "    if row['duplicate_flag'] =='D' and row['removal_flag'] == 'R':\n",
    "        geo = row['geometry']\n",
    "        for x in df['geometry']: \n",
    "            if geo == x:\n",
    "                return 'S'\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'S'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: combine_lat_lng_info\n",
    "\n",
    "def combine_lat_lng_info (row):\n",
    "    \n",
    "    lat_lng_list = []\n",
    "    \n",
    "    lat_lng_list.append(row['latitude'])\n",
    "    lat_lng_list.append(row['longitude'])\n",
    "    \n",
    "    return lat_lng_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: get_elevation_data\n",
    "\n",
    "def get_elevation_data (row, user_apikey):\n",
    "    \n",
    "        gmaps = googlemaps.Client(key=user_apikey)\n",
    "    \n",
    "        lat_lng_data = combine_lat_lng_info (row)\n",
    "\n",
    "        result = gmaps.elevation(locations=[lat_lng_data])\n",
    "        \n",
    "        return int(result[0]['elevation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error supression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I keep getting warning messages, which would tie up my eda cleaning processing. \n",
    "#     This would stop the warning, but this should not be used on a regualer bassis. Not a good practice.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data via Google places_nearby API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_location = '39.7392358, -104.990251' # Denver geo location lat:39.7392358  long:-104.990251\n",
    "user_radius = 50000                       # In meters: 5 miles = 8,046.72ft (use 1000 for test purpose) Max: 50,000\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 calls and 120 rows of data collected\n",
      "11 calls and 220 rows of data collected\n",
      "16 calls and 320 rows of data collected\n",
      "21 calls and 401 rows of data collected\n",
      "27 calls and 509 rows of data collected\n",
      "32 calls and 608 rows of data collected\n",
      "37 calls and 708 rows of data collected\n",
      "42 calls and 808 rows of data collected\n",
      "49 calls and 905 rows of data collected\n",
      "945 rows of data collected\n"
     ]
    }
   ],
   "source": [
    "saved_results = get_place_nearby (user_location, user_radius, place_type_list, apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results: Note use date at end of file name...\n",
    "\n",
    "saved_results.to_csv('../data/raw_data/places_nearby_results_rawdata_denver_co_20200220.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: Build FEMA Lifeline Database\n",
    "\n",
    "Processing Step:\n",
    "\n",
    "- Remove duplicates and reset index\n",
    "- Extract latitude and longitude info from geometry column and create latitude and longitude columns\n",
    "- Clean up types data. Remove unnessary data and cast data from string to list\n",
    "- Clean up plus_type data. Cast data from string to dict\n",
    "- Assign lifeline and category to data\n",
    "- Find and remove duplicate data across database\n",
    "- Call Google Elevation API to get elevation data for each buisnesses\n",
    "- Save database as clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve data from '../data/raw_data/places_nearby_results_rawdata_denver_co_20200220.csv' file\n",
    "df_place = pd.read_csv('../data/raw_data/places_nearby_results_rawdata_denver_co_20200220.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and reset index\n",
    "df_place.sort_values('geometry', inplace = True)\n",
    "df_place.drop_duplicates(subset = 'geometry', keep = 'first', inplace = True) \n",
    "df_place.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the geometry columns into latitude and longitude calculate the distance from point of orgin\n",
    "        \n",
    "user_location_lat = float(user_location.split(\",\")[0])\n",
    "user_location_lng = float(user_location.split(\",\")[1])\n",
    "\n",
    "df_place['latitude'] = df_place['geometry'].apply(lambda x: ast.literal_eval(x)['location']['lat'])\n",
    "df_place['longitude'] = df_place['geometry'].apply(lambda x: ast.literal_eval(x)['location']['lng'])\n",
    "\n",
    "df_place['distance'] = df_place['geometry'].apply(lambda x: get_distance(user_location_lat, \n",
    "                                                                         user_location_lng,\n",
    "                                                                         ast.literal_eval(x)['location']['lat'], \n",
    "                                                                         ast.literal_eval(x)['location']['lng']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean up the types data\n",
    "df_place['types'] = df_place['types'].apply(lambda x: cleanup_types(x, exclude_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign lifeline number and category section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign lifeline number and category to the data\n",
    "\n",
    "ndx = 1\n",
    "df_data = pd.DataFrame()\n",
    "df_sub = pd.DataFrame()\n",
    "\n",
    "while ndx <= len(fema_lifeline_dictionary):\n",
    "    for key, value in fema_lifeline_dictionary[ndx].items():\n",
    "        df_place['lifeline_number'] = df_place['types'].apply(lambda x: assign_lifeline(x, value, ndx))\n",
    "        \n",
    "        df_sub = df_place[df_place['lifeline_number'] == ndx]\n",
    "        df_sub['lifeline_category'] = key\n",
    "        \n",
    "        df_data = df_data.append(df_sub, sort=False, ignore_index=True)\n",
    "        \n",
    "        # clean_up\n",
    "        df_place.drop(columns='lifeline_number', inplace=True)\n",
    "        \n",
    "        ndx += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this\n",
    "df_data.to_csv('../data/raw_data/data_with_lifeline_info_rawdata_20200220.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: Duplicate rows\n",
    "\n",
    "The following routine will identify duplicate rows and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive data from '../data/raw_data/data_with_lifeline_info_rawdata_20200220.csv' file\n",
    "df_data = pd.read_csv('../data/raw_data/data_with_lifeline_info_rawdata_20200220.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of duplicate rows to pass to set_duplicate_flag\n",
    "df_dup = df_data[df_data.duplicated(subset='geometry', keep='first')]\n",
    "\n",
    "# set the duplicate flags \n",
    "df_data['duplicate_flag'] = df_data['geometry'].apply(lambda x: set_duplicate_flag(x, df_dup['geometry'].tolist()))\n",
    "\n",
    "# analyze the duplicate rows\n",
    "df_data['removal_flag'] = df_data.apply(lambda x: analyze_duplicate(x), axis = 1)\n",
    "\n",
    "# capture a subset of data where the rows have been marked duplicate and removal set to 'S'\n",
    "df_check = df_data[(df_data['duplicate_flag'] == 'D') & (df_data['removal_flag'] == 'S')]\n",
    "\n",
    "# call function to double check that all data with duplicate have been identified\n",
    "df_data['check_flag'] = df_data.apply(lambda x: double_check_duplicate_keys(x, df_check), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>name</th>\n",
       "      <th>types</th>\n",
       "      <th>vicinity</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>distance</th>\n",
       "      <th>lifeline_number</th>\n",
       "      <th>lifeline_category</th>\n",
       "      <th>duplicate_flag</th>\n",
       "      <th>removal_flag</th>\n",
       "      <th>check_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [geometry, name, types, vicinity, latitude, longitude, distance, lifeline_number, lifeline_category, duplicate_flag, removal_flag, check_flag]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final check for any duplicate row marked 'M'; missed\n",
    "df_data[df_data['check_flag'] == 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missed = df_data[df_data['check_flag'] == 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missed.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>name</th>\n",
       "      <th>types</th>\n",
       "      <th>vicinity</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>distance</th>\n",
       "      <th>lifeline_number</th>\n",
       "      <th>lifeline_category</th>\n",
       "      <th>duplicate_flag</th>\n",
       "      <th>removal_flag</th>\n",
       "      <th>check_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [geometry, name, types, vicinity, latitude, longitude, distance, lifeline_number, lifeline_category, duplicate_flag, removal_flag, check_flag]\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: Removal of duplicate rows\n",
    "\n",
    "If no rows are marked with 'M', it is safe to remove the duplicate rows flaged for removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicate rows across the lifelines\n",
    "df_data = df_data[df_data['removal_flag'] != 'R']\n",
    "\n",
    "# reset the index\n",
    "df_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# drop the duplicate's temp columns:'duplicate_flag', 'removal_flag', 'check_flag'\n",
    "df_data.drop(columns=['duplicate_flag', 'removal_flag', 'check_flag'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Geo info: Altitude/Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['elevation'] = df_data.apply(lambda row: get_elevation_data (row, apikey), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data[['geometry', 'latitude', 'longitude', 'distance', 'elevation', 'name', 'types', \n",
    "                  'vicinity', 'lifeline_number', 'lifeline_category' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data frame, cleaned.\n",
    "df_data.to_csv('../data/clean_data/fema_lifeline_info_clean_20200220.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
